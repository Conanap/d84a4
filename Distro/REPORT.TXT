CSC D84 - Artificial Intelligence

Assignment 4 - Neural Networks for OCR

This assignment is worth:

15 AIUs (Artificial Intelligence Units)
toward the 35% assignment component of your final
mark.

________________________________________________

Student Name 1 (last, first): Fung, Albion


Student Name 2 (last, first): Law, Chi Fai

Student number 1: 1002444321

Student number 2: 1002470444

UTORid 1: fungalbi

UTORid 2: lawchi1

READ THIS AND SIGN YOUR NAME AT THE END:

 I certify that I have read the UTSC code on academic
honesty and plaguarism. All work submitted as part
of this assignment is my own.

    Signed: _Albion Fung__      _Chi Fai Law_


(-5 marks for failing to provide the identifying
 information requested above)
________________________________________________

Answer the following questions. Be concise and clear
but explain carefully when needed.

1 .- (1 marks) Train a 1-layer network using the Logistic activation
               function. 

	       Copy/paste the reported classification rates for:
	 	
	       Training data (last training round):
              0: 92.9889
              1: 96.5157
              2: 78.4585
              3: 83.9216
              4: 87.8914
              5: 63.8636
              6: 92.2131
              7: 89.1304
              8: 77.3663
              9: 74.2004
              avg: 83.6550
              magnitude of largest network weight: 18.437312

	       Testing data:
              0: 95.9184
              1: 97.0044
              2: 79.2636
              3: 86.3366
              4: 87.7800
              5: 65.3587
              6: 92.1712
              7: 88.0350
              8: 78.0287
              9: 79.0882
              avg: 84.8985

2 .- (1 marks) Repeat the process using the hyperbolic tangent activation
	       function.

         Training data (last training round):
              0: 97.5207
              1: 95.4925
              2: 84.7222
              3: 85.1927
              4: 89.5492
              5: 81.1791
              6: 94.9612
              7: 92.1456
              8: 84.5361
              9: 84.4017
              avg: 88.9701
              Magnitude of largest network weight: 29.315943

         Testing data:
              0: 98.0612
              1: 97.1806
              2: 86.7248
              3: 88.4158
              4: 92.0570
              5: 82.2870
              6: 94.1545
              7: 90.7588
              8: 85.5236
              9: 84.1427
              avg: 89.9306


3 .- (1 marks) Which type of activation function yielded the best classification
	       results? which one learned faster?
          
          hyperbolic, hyperbolic (420000 (log) vs 345000 iterations (hyp))

4 .- (1 marks) Train a 2-layer network with hyperbolic-tangent activation, using
	       150 hidden units. Report the classification rates on training and
	       testing data just as for 1) and 2).	   

         Training data (last training round):
              0: 97.6789%
              1: 97.0332%
              2: 96.8254%
              3: 95.2891%
              4: 96.4803%
              5: 93.6471%
              6: 98.7395%
              7: 95.6685%
              8: 94.3205%
              9: 96.3611%
              avg: 96.3205%
              largest hidden to output weight: 12.531025
              largest input to hidden weight: 22.324726

         Testing data:
              0: 98.7755%
              1: 98.7665%
              2: 95.5426%
              3: 95.7426%
              4: 96.9450%
              5: 95.0673%
              6: 95.9290%
              7: 96.2062%
              8: 94.3532%
              9: 93.3598%
              avg: 96.0688%    
	       
5 .- (1 marks) Same as 4), except use 10 hidden units instead

         Training data (last training round):
              0: 96.1039%
              1: 96.8224%
              2: 87.4739%
              3: 89.2368%
              4: 91.5152%
              5: 82.9167%
              6: 93.1106%
              7: 92.5182%
              8: 87.0445%
              9: 90.5222%
              avg: 90.7265
              largest hidden to output weight: 6.549847
              largest input to hidden weight: 2.288707

         Testing data:
              0: 96.9388%
              1: 97.2687%
              2: 89.8256%
              3: 90.1980%
              4: 93.0754%
              5: 82.0628%
              6: 90.5010%
              7: 89.9805%
              8: 86.4476%
              9: 90.3865
              avg: 90.6685%   

6 .- (1 marks) Same as 5), except use 3 hidden units instead

         Training data (last training round):
              0: 93.7381
              1: 94.3534
              2: 80.1619
              3: 24.6654
              4: 78.1780
              5: 17.0732
              6: 49.2218
              7: 77.3389
              8: 16.6667
              9: 72.4858
              avg: 60.3883
              largest hidden to output weight: 4.539956
              largest input to hidden weight: 1.608933

         Testing data:
              0: 93.1633
              1: 97.1806
              2: 80.3295
              3: 23.6634
              4: 84.9287
              5: 31.6143
              6: 52.2965
              7: 73.9300
              8: 14.5791
              9: 81.2686
              avg: 63.2954

7 .- (4 marks) Comment on the experiments in 4, 5, and 6, and give your conclusion
	       regarding the effect of the number of hidden units on classification
	       accuracy. 

          higher hidden units results in much better accuracy, but too many hidden units gives marginal returns while increasing training time by a lot.

	       What is the network with 3 hidden telling you about this classification
	       problem?

         not enough hidden units results in poor accuracy due to lack of sampling.

8 .- (5 marks) Train the best performing network you can. Use any activation function
	       you wish, and set the number of hidden units so as to achieve the best
	       performance.

	       Number of hidden units used: 15, hyperbolic

	       Classification rate on testing data:
              0: 97.4490
              1: 96.5639
              2: 92.9264
              3: 88.7129
              4: 94.2974
              5: 81.1659
              6: 94.8852
              7: 88.6187
              8: 91.3758
              9: 91.3776
              avg: 91.7373
          (400000 iterations, better rates than 10 hidden units, only slightly worse than 150 hidden units)

9 .- (5 marks) Describe how you would build a network to play the cat
	        and mouse game (move the mouse to help it survive).

		- Describe what the input is in terms of a vector of
		  values
        Inputs potential results of next step for mouse, i.e. what would happen to it after it takes a step left/right/up/down next turn (assuming legitimate move).
		- Describe what the output layer looks like (how many
		  neurons and what they encode)
        4 neurons, outcome of corresponding step in terms of overall game winrate
		- Describe the error function
        expected result from move (i.e. eat cheese, win) vs actual result of move
		- How many layers should you use?
        2
_____________________________________________________

Mark with an 'x' where appropriate. If something is only
working partially, briefly describe what works, what
doesn't work, or what problems exist.
	
        	Complete/Working	Partial		Not done

Logistic
 activation         x
 
Tanh
 activation         x

1L Feed-forward     x 

1L Backprop         x

1L Classify         x

2L Feed-forward     x

2L Backprop         x

2L Classify         x
_____________________________________________________

Marking:

(15) one-layer, logistic network trains and classifies

(5)  one-layer, tanh network trains and classifies

(25) two-layer, tanh network trains and classifies

(5) two-layer, logistic network trains and classifies

(5 marks) Can train 2-layer networks with either activation function
	  and any number of hidden units

(20 marks) Answers in this report

Total for A4:       / out of 75
